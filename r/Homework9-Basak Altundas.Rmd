---
title: 'STA 5207: Homework 9'
date: "Due: Tuesday, November 19th by 11:59 PM"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

Include your R code in an R chunks as part of your answer. In addition, your written answer to each exercise should be self-contained so that the grader can determine your solution without reading your code or deciphering its output.

## Exercise 1 (Brains) [40 points]

For this exercise, we will use the `mammals` data set in the `MASS` package. You can also find the data in `mammals.csv` on Canvas. The data set contains the average brain and body weights of 62 species of land mammals. There are 62 observations and two variables:

-   `body`: Average body weight in kilograms (kg).
-   `brain`: Average brain weight in grams (g).

In the following exercise, we will use `brain` as the response and `body` as the predictor.

1.  (5 points) Perform OLS regression with `brain` as the response and `body` as the predictor. Check the normality and constant variance assumptions using a hypothesis test at the $\alpha = 0.05$ level. Do you feel that they have been violated? Justify your answer.

    ```{r}
    library(MASS)
    data("mammals")
    model=lm(brain~body, data=mammals)

    ```

    ```{r}
    library(lmtest)

    bptest(model)
    ```

    The null and alternative hypotheses are $H_0$: Homoscedastic errors and $H_1$:Heteroscedastic errors. Since the $p$-value is 0.0003989 which is less than the significance level $\alpha$= 0.05 , we reject the null hypothesis. We conclude that the constant variance assumption is violated.

    ```{r}
    shapiro.test(resid(model))
    ```

    The $p$-value is $2.316\times10^{-14}$, which is less than the significance level of 0.05. We reject the null hypothesis and conclude that the errors have a non-normal distribution.

2.  (3 points) Create a scatter plot of the data and add the fitted regression line. Based on this plot, does their appear to be any outliers, high-leverage points, or high influential data points? Include the plot in your response.

    ```{r}
    plot(brain~body, data = mammals,
    xlab = "Average body weight in kilograms (kg)",
    ylab = "Average brain weight in grams (g)",
    main = "Brain vs. Body",
    pch = 20, cex = 2,
    col = "gray")
    abline(model, lwd = 3, col = "darkorange")
    ```

    From the scatter plot provided, it appears that there are a few potential outliers and possibly high-leverage points.

3.  (6 points) Since the body weights range over more than one order of magnitude and are strictly positive, we will use $\log(\texttt{body})$ as our *predictor*, with no further justification (Recall *the log rule*: if the values of a variable range over more than one order of magnitude and the variable is strictly positive, then replacing the variable by its logarithm may be helpful). Use the Box-Cox method to verify that $\log(\texttt{brain})$ is then a "recommended" transformation of the *response* variable. That is, verify that the log transformation is amoung the "recommended" values of $\lambda$ when considering, $$
    g_{\lambda}(\texttt{brain}) = \beta_0 + \beta_1 \log(\texttt{body}) + \varepsilon_i.
    $$ Report the relevant plot returned by the `boxcox` function and use the appropriate zoom onto the relevant values. Indicating the property of the plot that justifies the $\log$ transformation.

    ```{r}
    model_x=lm(brain~log(body), data=mammals)
    bc = boxcox(model_x, lambda = seq(-0.1, 0.1, by = 0.05), plotit = TRUE)

    ```

    We should always pick a meaningful value from the CI. Since the zero value lies within the confidence interval, the Box-Cox method justifies our choice of a log transform.

4.  (5 points) Fit the model justified in Question 3. That is, fit a model with $\log(\texttt{brain})$ as the response and $\log(\texttt{body})$ as the predictor. Create a scatter plot of the data and add the fitted regression line for this model. Does a linear relationship seem to be appropriate here?

    ```{r}
    model_bc=lm(log(brain)~log(body), data=mammals)
    plot(log(brain)~log(body), data = mammals,
    xlab = "Log-Transformed Average body weight in kilograms (kg)",
    ylab = "Log-Transformed Average brain weight in grams (g)",
    main = "Log-Transformed Brain vs. Log-Transformed Body",
    pch = 20, cex = 2,
    col = "gray")
    abline(model_bc, lwd = 3, col = "darkorange")

    ```

    From the scatter plot provided, a linear relationship appears to be appropriate. The points are distributed around the fitted regression line.

5.  (3 points) Based on the model from Question 4, check the normality and constant variance assumptions using a hypothesis test at the $\alpha = 0.05$ level. Do you feel that they have been violated? Justify your answer.

    ```{r}
    bptest(model_bc)
    shapiro.test(resid(model_bc))
    ```

    The $p$-values for both tests are greater than the significance level of 0.05. We do not reject the null hypothesis. We conclude that the errors have a normal distribution and constant variance.

6.  (6 points) Using the model from Question 4, check for any high influential observations. Report any observations you determine to be highly influential.

    ```{r}
    which(cooks.distance(model_bc) > 4 / length(cooks.distance(model_bc)))
    ```

    We find one influential point; observation 32.

7.  (6 points) Use the model in Question 4 to predict the brain weight of a male Snorlax, which has a body weight of 1014.1 *pounds*. (A Snorlax would be a mammal, right?) Construct a 90% prediction interval.

    ```{r}
    new_data = data.frame(body = 1014.1 * 0.453592)
    trans_preds = predict(model_bc,  newdata=new_data, interval = 'prediction', level=0.90)
    prediction= exp(trans_preds)
    print(prediction)

    ```

    Based on this result, the predicted value is 848.5468 with a 90% prediction interval of (257.8228 , 2792.738).

8.  (6 points) A common measure of model performance is the root mean squared error (RMSE), which is defined as $$
    \mathsf{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}.
    $$ We prefer models with a lower $\mathsf{RMSE}$. Report the $\mathsf{RMSE}$ values for the model in Question 1 and Question 4. Based on this criteria, which model do you prefer?

    ```{r}
    sqrt(mean((mammals$brain - predict(model))^2))
    ```

    ```{r}
    sqrt(mean((mammals$brain - exp(predict(model_bc)))^2))
    ```

    In this case, the model with the lowest RMSE is the model using the log transformation. In other words, we prefer the model from Question 4.

## Exercise 2 (TV and Health) [40 points]

For this exercise, we will use the `tvdoctor` data set in the `faraway` package. You can also find the data in `tvdoctor.csv` on Canvas. The data set contains information on life expectancy, doctors, and televisions collected in 38 countries in 1993. There are 38 observations on three variables:

-   `life`: Life expectancy in years.
-   `tv`: Number of people per television set.
-   `doctor`: Number of people per doctor.

In the following exercise, we will use `life` as the response and `tv` as the predictor.

1.  (6 points) Use forward selection based on $t$-tests at the $\alpha = 0.05$ level to select a $d$-th degree polynomial model with `life` as the response and `tv` as the predictor. Report the estimated regression equation for your chosen model.

    ```{r}
    library(faraway)
    data("tvdoctor")
    model_poly1 = lm(life ~ tv, data = tvdoctor)

    summary(model_poly1)
    ```

    We see that the $p-$value for the linear term is less than 0.05. Therefore, the linear term is significant and we move on to fitting a quadratic model.

    ```{r}
    model_poly2 = lm(life ~ poly(tv, 2, raw = TRUE), data = tvdoctor)

    summary(model_poly2)
    ```

    The $p-$value for the quadratic term is less than 0.05. Therefore the quadratic term is significant, and we move on to the cubic term.

    ```{r}
    model_poly3 = lm(life ~ poly(tv, 3, raw = TRUE), data = tvdoctor)

    summary(model_poly3)
    ```

    The $p-$value for the cubic term is not significant, so we stick with the quadratic model.

    The estimated regression equation for the 2nd degree polynomial model is $life_i=71.06-0.1285tv_i+0.000018tv_i^2$

2.  (6 points) Fit polynomial models of degree 1 and 2. Create a scatter plot of the data and add the fitted regression line for each polynomial model. Include the plot in your response.

    ```{r}

    plot(life ~ tv, data = tvdoctor, 
         xlab = 'Number of people per television set', ylab = 'Life expectancy in years',
         pch = 20, cex = 2, col = 'gray', 
         xlim = c(0, 600), ylim = c(40, 80))

    abline(model_poly1, col = 'darkorange', lty = 'dashed', lwd = 2)

    xplot = seq(0, 600, by = 10)
    lines(xplot, predict(model_poly2, newdata = data.frame(tv = xplot)),
          col = "black", lwd = 2)

    legend("topright", title = "degree", cex = 0.8,
           legend = c("d = 1", "d = 2"), 
           lwd = 2, lty = c(2, 1), 
           col = c("darkorange", "black"))
    ```

3.  (3 points) Check for any high *leverage* points using the quadratic model you fit in Question 2. Report any observations you determine to have high leverage.

    ```{r}
    high_lev_ids = which(hatvalues(model_poly2) > 2 * mean(hatvalues(model_poly2)))
    high_lev_ids

    ```

    In this case, we see that three countries, Bangladesh, Ethiopia and Myanmar, have high leverage.

4.  (6 points) Use forward selection based on $t$-tests at the $\alpha = 0.05$ level to select a $d$-th degree polynomial model with `life` as the response and `tv` as the predictor with the high *leverage* data points you identified in Question 3 removed. Report the estimated regression equation for your chosen model.

    ```{r}
    non_inf_ids = which(hatvalues(model_poly2) <= 2 * mean(hatvalues(model_poly2)))

    model_poly1_fix = lm(life ~ tv, data = tvdoctor, subset = non_inf_ids)

    summary(model_poly1_fix)
    ```

    The $p-$value of `tv` is significant, so we move on the quadratic term.

    ```{r}
    model_poly2_fix = lm(life ~ poly(tv, 2, raw=TRUE), data = tvdoctor, subset = non_inf_ids)

    summary(model_poly2_fix)
    ```

    The $p-$value of $tv^2$ is significant, so we move on the cubic term.

    ```{r}
    model_poly3_fix = lm(life ~ poly(tv, 3, raw=TRUE), data = tvdoctor, subset = non_inf_ids)

    summary(model_poly3_fix)
    ```

    The $p-$value for the cubic term is not significant, so we stick with the quadratic model.

    The estimated regression equation for the 2nd degree polynomial model is

    $life_i=75.8351-0.7909tv_i+0.0073tv_i^2$

5.  (6 points) Fit polynomial models of degree 1 and 2 with the high *leverage* data points you identified in Question 3 removed. Create a scatter plot of the data (**Note**: use the `subset` argument to `plot`) and add the fitted regression line for each polynomial model. Include the plot in your response.

    ```{r}
    plot(life ~ tv, data = tvdoctor, subset = non_inf_ids, 
         xlab = 'Number of people per television set', ylab = 'Life expectancy in years',
         pch = 20, cex = 2, col = 'gray', 
         xlim = c(0, 200), ylim = c(40, 100))

    abline(model_poly1_fix, col = 'darkorange', lty = 'dashed', lwd = 2)

    xplot = seq(0, 600, by = 10)
    lines(xplot, predict(model_poly2_fix, newdata = data.frame(tv = xplot)),
          col = "black", lwd = 2)

    legend("topright", title = "degree", cex = 0.8,
           legend = c("d = 1", "d = 2"), 
           lwd = 2, lty = c(2, 1), 
           col = c("darkorange", "black"))
    ```

6.  (5 points) Since the number of people per television set (`tv`) ranges over more than one order of magnitude and are strictly positive, we might use $\log(\texttt{tv})$ as our predictor. Fit an OLS regression model with `life` as the response and $\log(\texttt{tv})$ as the predictor. Report the estimated regression equation for this model.

    ```{r}
    model_xlog = lm (life~log(tv), data=tvdoctor)
    summary (model_xlog)
    ```

    The estimated regression equation is

    ${\widehat{life_i}} = 77.8873 - 4.2597\times {log(tv)}_i$

7.  (3 points) Create a scatter plot of the `life` vs $\log(\texttt{tv})$ and add the fitted regression line for model you fit in Question 6. Include the plot in your response.

    ```{r}
    plot(life~log(tv), data=tvdoctor,
    xlab = "Log-Transformed Number of people per television set",
    ylab = "Life expectancy in years",
    main = "Life vs Log-tv",
    pch = 20, cex = 2,
    col = "gray")
    abline(model_xlog, lwd = 3, col = "darkorange")
    ```

    The points in the scatter plot appear to fit a linear regression line reasonably well.

8.  (5 points) Report the adjusted $R^2$ values for the quadratic model you fit in Question 2 and the model you fit in Question 6. Based on this criteria, which model do you prefer?

    ```{r}
    summary(model_poly2)$adj.r.squared
    summary(model_xlog)$adj.r.squared
    ```

    Model in Question 6 is preferred because it has a significantly higher adjusted $R^2$.

## Excercise 3 (The `cars` Data Set) [20 points]

For this exercise, we will use the built-in `cars` data set. You can also find the data in `cars.csv` on Canvas. In the following exercise, we will use `dist` as the response and `speed` as the predictor.

1.  (5 points) Perform OLS regression with `dist` as the response and `speed` as the predictor. Check the normality and constant variance assumptions using a hypothesis test at the $\alpha = 0.05$ level. Do you feel that they have been violated? Justify your answer.

    ```{r}
    data(cars)
    model=lm(dist~speed, data=cars)
    ```

    ```{r}
    library(lmtest)

    bptest(model)
    shapiro.test(resid(model))

    ```

    Since the p-value 0.07297 is greater than 0.05, we fail to reject the null hypothesis.The assumption of constant variance is not violated; the residuals appear to have constant variance (homoscedasticity).

    Since the p-value 0.02152 is less than 0.05, we reject the null hypothesis at the 0.05 significance level. The residuals do not appear to follow a normal distribution. Normality is violated in this case.

2.  (10 points) Use the Box-Cox method to verify that $\sqrt{\texttt{dist}}$ is a "recommended" transformation of the *response* variable. That is, verify that the square-root transformation is among the "recommended" values of $\lambda$ when considering, $$
    g_{\lambda}(\texttt{dist}) = \beta_0 + \beta_1 \texttt{speed} + \varepsilon_i.
    $$ Report the relevant plot returned by the `boxcox` function and use the appropriate zoom onto the relevant values. Indicating the property of the plot that justifies the square-root transformation.

    ```{r}
    library(MASS)

    bc = boxcox(model, lambda = seq(0, 0.90, by = 0.10), plotit = TRUE)
    ```

    We should always pick a meaningful value from the CI. Since the 0.50 value lies within the confidence interval, the Box-Cox method justifies our choice of a square-root transform.

3.  (5 points) Fit the model justified in Question 2. That is, fit a model with $\sqrt{\texttt{dist}}$ as the response and $\texttt{speed}$ as the predictor. Check the normality and constant variance assumptions using a hypothesis test at the $\alpha = 0.05$ level. Do you feel that they have been violated? Justify your answer.

    ```{r}
    model_bc = lm(dist ^ 0.5 ~ speed, data = cars)

    bptest(model_bc)

    shapiro.test(resid(model_bc))

    ```

    Since the p-values for both tests are greater than 0.05, we fail to reject the null hypothesis.The assumption of constant variance and normality is not violated. The residuals appear to have constant variance and follow a normal distribution.
